<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="keywords" content="9p, 9p2000, research, Plan 9, Inferno, Unix, Open Source, Bell Labs" />
	<meta name="description" content="Open source distributed operating system Plan 9 from Bell Labs." />
	<link rel="stylesheet" href="/wiki-style.css" media="all" type="text/css" />
	<title>Hubfs (as of Sat Mar  2 23:10:45 EST 2013)</title>
</head>
<body>
<center>
<h1><a name="top"></a>Hubfs</h1>
<h2><i>-as of Sat Mar  2 23:10:45 EST 2013-</i></h2>
<p />
</center>

<div class='body'> <!-- ***************************** -->

Hubfs
is
a
9p
fs
which
can
be
used
similarly
to
programs
like
tmux
and
GNU
screen.
Unlike
those
programs
it
allows
shells
from
multiple
machines
to
be
shared
to
a
single
hub
and
enables
free
piping
of
data
between
processes
on
different
machines.
(Screen
and
tmux
need
an
additional
tool
like
ssh
to
connect
multiple
machines
and
have
no
ability
to
multiplex
raw
pipes.)
Due
to
the
use
of
9p
as
the
interface,
hubfs
brings
shell
inputs
and
outputs
across
a
network
of
machines
into
the
file
namespace.
<p class='para'>
Hubfs
is
found
on
the
sources
server
in
contrib/mycroftiv/hubfs1.1.
It
can
also
be
installed
using
fgb's
contrib
with:
</p>

<pre>
contrib/install mycroftiv/hubfs
</pre>

<a name="BASIC_USE" /><h3>BASIC USE</h3>

<p class='para'>
The
"hub"
command
both
creates
and
attaches
to
hubfs
shells.
To
start
a
new
hubfs
just
type
</p>

<pre>
hub
</pre>

<p class='para'>
to
create
a
new
persistent
shell.
Hubfs
uses
/srv/hubfs
as
its
default
name.
To
connect
to
a
running
hubfs,
the
same
</p>

<pre>
hub
</pre>

<p class='para'>
command
will
mount
and
attach
to
an
existing
/srv/hubfs.
To
connect
to
a
hubfs
shell
on
a
remote
machine,
you
need
access
to
that
machine's
/srv.
From
a
client
who
wishes
to
connect
to
an
existing
shell
on
the
server:
</p>

<pre>
import -a server /srv
hub
</pre>

<p class='para'>
By
making
the
remote
machine's
/srv/hubfs
visible
in
the
namespace,
the
hub
command
will
connect
as
usual.
</p>

<p class='para'>
The
first
parameter
to
the
"hub"
command
is
the
name
of
the
hubfs.
If
you
wish
to
use
a
name
other
than
/srv/hubfs
the
command
</p>

<pre>
hub name
</pre>

<p class='para'>
creates
a
new
hubfs
at
/srv/name.
Following
this,
</p>

<pre>
hub name
</pre>

<p class='para'>
will
attach
to
the
hubfs
at
/srv/name.
</p>

<a name="LOCAL_AND_REMOTE_SHELLS_AND_MOVING_BETWEEN_THEM" /><h3>LOCAL AND REMOTE SHELLS AND MOVING BETWEEN THEM</h3>

<p class='para'>
The
"hub"
command
creates
a
default
shell
named
"io"
to
begin.
When
you
are
working
within
a
connected
shell,
you
can
create
new
shells.
These
shells
can
be
created
on
either
the
machine
hosting
the
hubfs
or
on
the
client
machine.
In
other
words,
you
have
the
option
of
"sharing
a
shell
back"
to
the
hubfs
server
from
your
machine.
On
a
grid
of
machines,
each
client
will
usually
share
a
local
shell
back
to
the
central
hubfs.
The
following
commands
are
issued
from
within
a
connected
shell:
</p>

<pre>
%remote name
</pre>

<p class='para'>
Creates
and
connects
to
a
new
shell
running
on
the
hubfs
host.
</p>

<pre>
%local name
</pre>

<p class='para'>
Creates
and
moves
to
a
new
shell
on
the
local
machine
and
shares
that
shell
to
hubfs.
</p>

<pre>
%attach name
</pre>

<p class='para'>
Moves
between
existing
shells
attached
to
the
hubfs.
It
is
also
possible
to
connect
to
existing
subshells
within
a
hubfs
or
create
new
ones
directly
from
an
external
shell
prompt.
</p>

<pre>
hub srvname shellname
</pre>

<p class='para'>
Acts
to
attach
to
the
given
shellname
within
the
hubfs
at
srvname,
or
create
and
attach
shellname
to
that
hubfs
if
an
existing
subshell
of
that
name
is
not
present.
</p>

<a name="CONNECTING_OTHER_OPERATING_SYSTEMS" /><h3>CONNECTING OTHER OPERATING SYSTEMS</h3>

<p class='para'>
Hubfs
is
client
agnostic.
If
a
system
can
use
9p,
it
can
mount
hubfs
and
access
shells
within
it,
and
even
share
shells
back.
Here
is
an
example
using
plan9port
and
9pfuse.
These
actions
would
usually
be
automated
by
small
scripts.
</p>

<p class='para'>
On
the
Plan
9
host:
</p>

<pre>
hub -b unix
mount -c /srv/unix /n/unix
touch /n/unix/un0 /n/unix/un1 /n/unix/un2
aux/listen1 -tv tcp!*!8787 /bin/exportfs -r /n/unix
</pre>

<p class='para'>
To
connect
a
client
machine
such
a
linux
box
running
p9p:
</p>

<pre>
mkdir hubfs
9pfuse 'tcp!server.ip!8787' hubfs
rc -i &lt;hubfs/un0 &gt;&gt;hubfs/un1 2&gt;&gt;hubfs/un2 &amp;
</pre>

<p class='para'>
This
shares
a
shell
from
the
linux
box
back
to
the
hubfs.
At
this
point
the
listen
command
on
the
server
can
be
terminated.
The
linux
box
can
now
be
controlled
from
any
plan
9
system
which
can
mount
the
hubfs.
To
control
the
Plan
9
system
from
linux:
</p>

<pre>
cat hubfs/io1 &amp;
cat hubfs/io2 &amp;
cat &gt;&gt;hubfs/io0
</pre>

<p class='para'>
And
you
are
now
attached
to
and
controlling
the
initial
Plan
9
hubfs
shell.
To
attach
to
the
shell
on
the
linux
machine
from
within
a
Plan
9
system
with
access
to
the
/srv/unix
:
</p>

<pre>
hub unix un
</pre>

<p class='para'>
Will
connect
to
the
shell
running
on
the
linux
box.
</p>

<a name="GENERAL_PURPOSE_HUBFS:_MULTIPLEXED_NETWORK_PIPES" /><h3>GENERAL PURPOSE HUBFS: MULTIPLEXED NETWORK PIPES</h3>

<p class='para'>
The
examples
just
shown
illustrate
that
behind
the
scenes,
hubfs
itself
simply
multiplexes
pipe-like
files.
This
approach
works
for
"screen-like"
functionality
in
Plan
9,
because
there
is
no
TTY
layer
to
manage.
Multiplexing
access
to
a
shell
can
be
done
efficiently
simply
by
creating
pipelike
buffers
for
each
file
descriptor
and
allowing
multiple
readers
and
writers
access
to
these
files
via
9p.
</p>

<p class='para'>
This
is
actually
a
powerful
general
purpose
mechanism
which
can
be
used
for
more
than
just
shells.
Any
textual/bytestream
application
or
data
stream
can
be
attached
to
hubfs
to
multiplex
access
and
persist
the
data.
This
can
be
used
to
create
grid
processing
pipelines
using
shell
commands
with
no
additional
clustering
layer.
For
instance,
suppose
there
is
a
textual
data
stream
and
it
is
desired
to
search
this
data
stream
for
a
number
of
different
terms,
then
create
a
new
data
stream
that
is
the
composite
of
all
the
chosen
matches.
</p>

<pre>
mount -c /srv/hubfs /n/hubfs
touch /n/hubfs/streamin /n/hubfs/streamout
cat /lib/words &gt;&gt;/n/hubfs/streamin
</pre>

<p class='para'>
Each
processing
node
on
the
grid
issues
commands
such
as
</p>

<pre>
import -a hubserver /srv
mount /srv/hubfs /n/hubfs
grep -b foo /n/hubfs/streamin &gt;&gt;/n/hubfs/streamout
</pre>

<p class='para'>
The
next
machine
greps
for
"bar",
the
next
machine
greps
for
"baz",
etc.
The
result
of
this
is
an
automatic
"fan
out"
of
the
greps
for
different
terms
to
different
processing
nodes
and
then
creation
of
an
output
data
stream
at
"streamout"
which
consists
of
all
matches
found
by
all
nodes.
</p>

<a name="HUBFS_FOR_POWER_USERS:_FREEZE/MELT,_DEVICE_SAVING,_TRICKS" /><h3>HUBFS FOR POWER USERS: FREEZE/MELT, DEVICE SAVING, TRICKS</h3>

<p class='para'>
Hubfs
has
some
features
and
related
scripts
which
offer
additional
capabilities
for
users.
The
most
notable
is
probably
the
ability
to
"freeze"
the
flow
of
data
in
the
hubfiles
and
allow
them
to
treated
as
random-access
static
files,
then
"melt"
them
to
resume
the
active
flow
of
data.
This
enables
the
following
workflow:
</p>

<pre>
mount /srv/hubfs /n/hubfs
echo freeze &gt;/n/hubfs/ctl
tar czf hublog.tgz /n/hubfs
echo melt &gt;/hubfs/ctl
</pre>

<p class='para'>
The
result
of
this
is
that
the
buffer
contents
(the
input/output
history)
of
every
file
descriptor
attached
to
the
hubfs
is
saved
as
a
tarball.
If
you
look
at
this
tarball,
each
file
descriptor
for
each
shell
will
have
a
file
with
its
stream
-
so
the
io0
file
contains
the
user
input,
the
io1
file
contains
the
standard
output,
and
the
io2
file
contains
the
standard
error
output.
</p>

<p class='para'>
One
standard
use
of
this
functionality
would
be
to
log
irc
channels
where
the
irc
programs
are
running
inside
hubfs.
The
author
uses
irc7
and
the
irc
client
from
contrib/andrey
and
periodically
updates
logs
with
a
freeze/melt
cycle
of
the
hubfs
containing
4
different
irc
channels.
</p>

<p class='para'>
An
issue
users
may
notice
is
that
programs
such
as
"lc"
which
need
to
know
the
width
of
the
window
they
are
running
within
will
not
know
about
the
width
of
the
window
a
shell
within
hubfs
is
running.
This
issue
can
be
dealt
with
by
the
use
of
"device
saving"
and
"device
retrieval"
which
has
many
applications.
contrib/mycroftiv/rootlessnext/scripts
contains
the
"savedevs"
and
"getdevs"
scripts
which
can
be
used
to
connect
a
shell
within
hubfs
to
the
active
window
to
enable
programs
such
as
"lc"
to
run
correctly,
and
also
enable
the
use
of
some
graphical
programs,
although
they
will
not
be
multiplexed
to
other
clients.
Example
usage,
presuming
a
hubfs
already
exists
and
is
accessible
to
be
used:
</p>

<pre>
savedevs myterm
hub
%io getdevs myterm
</pre>

<p class='para'>
and
now
the
shell
within
the
hubfs
will
be
aware
of
the
current
window
size
and
track
its
changes
properly.
</p>

<p class='para'>
An
example
of
the
kind
of
trick
which
hubfs
enables
with
some
practical
and
impractical
uses:
</p>

<pre>
grep -b FOO /n/myirc/plan9chan1 |while(echo `{read} &gt;&gt;/tmp/foolog) echo 'echo THERES ONE' &gt;&gt;/n/myirc/plan9chan0
</pre>

<p class='para'>
That
command
is
used
on
hubs
running
an
irc
client.
It
monitors
the
channel
for
mentions
of
FOO,
logs
them
to
a
file,
and
makes
the
user
shout
THERES
ONE
into
the
channel
whenever
FOO
is
uttered.
A
similar
concept
can
be
used
to
do
things
such
as
trigger
auto-plumb
of
.png
files
to
the
plumber
when
directories
containing
them
are
ls.
</p>

<a name="STDERR_SYNCRHONIZATION" /><h3>STDERR SYNCRHONIZATION</h3>

<p class='para'>
The
simplicity
of
multiplexing
shells
just
by
buffering
and
multiplexing
their
file
descriptors
provides
many
benefits,
but
does
have
a
few
unexpected
consequences.
One
such
is
the
lack
of
enforced
synchronization
between
shell
file
descriptors.
When
a
large
amount
of
data
is
transmitted
on
stdout
and
a
small
amount
on
stderr,
it
is
a
natural
consequence
of
the
parallelism
and
independence
of
the
client
readers
that
the
client
will
finish
reading
the
messages
from
stderr
before
finishing
reading
the
the
messages
sent
from
stdout.
This
is
expected
behavior
but
contrary
to
user
expectation
that
the
shell
prompt
will
only
appear
after
the
output
of
a
command
has
been
completed.
To
address
this,
the
user
can
set
a
delay
on
a
given
file
descriptor
in
the
hubshell
client.
</p>

<pre>
%err 300
</pre>

<p class='para'>
Will
add
300
milliseconds
of
delay
before
printing
messages
from
stderr
on
that
shell.
This
usually
improves
the
user
experience
when
working
with
a
remote
machine
over
WAN.
There
is
no
negative
consequence
from
these
seemingly
misplaced
prompts,
commands
are
received
and
sent
normally
regardless
of
the
visual
location
of
the
prompt
in
the
output
stream
as
seen
by
the
client
on-screen.
The
lack
of
synchronization
is
client-side
and
the
result
of
parallelized
reads
-
the
prompt
or
error
messages
are
always
printed
at
the
normal
time
on
the
server
end.
</p>

<a name="PARANOID_MODE" /><h3>PARANOID MODE</h3>

<p class='para'>
Paranoid
mode
is
a
nonstandard
mode
of
operation
designed
for
the
transmission
of
large
continuous
data
streams.
Hubfs
is
optimized
for
the
type
of
data
transmission
typical
of
shell
usage
-
bursts
of
data,
where
the
size
of
a
single
burst
is
less
than
500kb
and
often
much
smaller.
It
prioritizes
interactivity
and
provides
a
semantics
where
writers
are
allows
allowed
to
write
without
needing
to
wait
for
client
readers
to
read
data
previously
sent.
This
is
different
than
conventional
blocking
write
semantics
on
pipes.
</p>

<p class='para'>
As
a
result,
normal
hubfs
operation
is
often
unsuited
to
a
scenario
where
you
are
sending
a
continuous
stream
of
data
of
a
size
of
megabytes
or
more.
The
hazard
is
that
writers
will
write
data
faster
than
remote
readers
can
read
it,
and
the
writer
will
overwrite
the
portion
of
the
buffer
that
the
client
has
not
yet
read,
resulting
in
corruption
of
the
data
stream
and
loss
of
data.
"paranoid"
mode
is
provided
to
avoid
this.
In
the
event
it
is
desired
to
send
a
large
continuous
stream
of
data
much
larger
than
the
hubfs
static
circular
buffer
of
700k,
the
command
</p>

<pre>
echo fear &gt;/n/hubfs/ctl
</pre>

<p class='para'>
puts
hubfs
into
paranoid
mode,
where
the
speed
of
writes
is
gated
by
the
ability
of
readers
to
catch
up.
This
mode
incurs
a
considerable
performance
penalty
and
cpu
usage
tax
and
is
not
needed
for
normal
use
by
interactive
shells.
</p>

<pre>
echo calm &gt;/n/hubfs/ctl
</pre>

<p class='para'>
returns
hubfs
to
standard
operation.
</p>

<a name="GRIO_AND_OTHER_RELATED_TOOLS" /><h3>GRIO AND OTHER RELATED TOOLS</h3>

<p class='para'>
To
make
access
to
the
persistent
shells
with
hubfs
as
easy
as
possible,
the
modified
<a href="http://plan9.bell-labs.com/magic/man2html/1/rio"><i>rio</i>(1)</a>
grio
(found
at
sources/contrib/mycroftiv/grio)
automatically
creates
/srv/riohubfs.$user
and
provides
a
new
menu
option,
Hub,
to
sweep
out
a
window
in
the
same
manner
as
New,
but
rather
than
creating
a
new
<a href="http://plan9.bell-labs.com/magic/man2html/1/rc"><i>rc</i>(1)</a>,
it
connects
to
the
existing
hubfs
session.
The
effect
is
that
when
you
make
a
new
window,
you
are
'back
where
you
were
before'.
The
standard
New
options
remains
as
before.
</p>

<p class='para'>
On
a
grid
of
machines,
if
the
central
cpu
server
hosts
the
main
hubfs,
and
user
includes
</p>

<pre>
import -a cpuserver /srv &amp;
</pre>

<p class='para'>
in
their
profile,
then
when
grio
is
started,
it
will
automatically
make
use
of
that
pre-existing
central
hubfs.
Assuming
all
nodes
also
use
%local
to
share
shells
back
to
the
central
hubfs
server,
this
gives
the
user
persistent
access
to
shells
on
machines
on
each
node
of
their
grid
available
directly
from
the
creation
of
a
new
rio
window.
</p>

<p class='para'>
Hubfs
and
grio
are
independent
parts
of
a
larger
toolkit
of
Plan
9
namespace
software
found
in
sources/contrib/mycroftiv/rootlessnext
and
with
full
documentation
and
papers
at
<a href="http://ants.9gridchan.org">ANTS</a>.
</p>

<a name="ARCHITECTURE_AND_INTERNALS" /><h3>ARCHITECTURE AND INTERNALS</h3>

<p class='para'>
Mostly
unmentioned
in
this
discussion
has
been
the
hubshell
client,
which
is
conventionally
used
for
creating
and
moving
between
hubfs
shells.
Hubfs
itself
simply
creates
pipe-like
files.
It
is
not
aware
of
what
applications
are
being
connected
to
the
files,
it
simply
provides
buffers
and
answers
read
and
write
requests.
Hubfs.c
itself
is
deliberately
simplistic
and
was
evolved
from
the
sample
ramfs
implementation
in
the
distribution
at
/sys/src/lib9p/ramfs.c.
</p>

<p class='para'>
Hubshell.c
is
the
main
client
of
hubfs.
The
%commands
are
intercepted
and
handled
by
hubshell,
which
knows
how
to
create
new
<a href="http://plan9.bell-labs.com/magic/man2html/1/rc"><i>rc</i>(1)</a>
shells
attached
to
hubfs
and
to
move
between
them.
In
the
standard
mode
of
use,
there
are
several
chains-of-cats
between
the
user
and
the
shell.
The
input
sent
by
the
users
(and
output
they
read)
is
actually
that
of
hubshell,
which
forks
3
cats
to
transfer
data
between
itself
and
the
hubfiles
-
and
those
hubfiles
are
essentially
"buffered
cats"
connected
to
an
rc.
</p>

<p class='para'>
The
hub
wrapper
script
checks
the
namespace
at
/n/name
or
/srv/name
for
an
available
hubfs
and
creates
a
new
hubfs
if
necessary,
then
starts
a
hubshell
connected
to
the
target.
</p>

<p class='para'>
Internally,
hubfs
creates
a
statically
sized
buffer
for
each
hubfile,
then
writers
write
data
circularly
from
beginning
to
end
and
wrap
around
when
they
fill
the
buffer.
Each
hub
maintains
two
"spindles"
of
9p
requests,
one
for
reads
and
one
for
writes.
Writes
are
always
appended
but
the
location
of
each
reader
needs
to
be
tracked
individually.
Hubfs
is
usually
single-process
and
single-threaded,
but
paranoid
mode
changes
this
and
in
paranoid
mode
hubfs
uses
rfork
and
qlocks
to
separate
the
flow
of
control
handling
write
requests
from
that
handling
read
requests.
</p>

<p class='para'>
The
pre-existing
<a href="http://plan9.bell-labs.com/magic/man2html/4/consolefs"><i>consolefs</i>(4)</a>
in
the
standard
distribution
has
a
fairly
similar
architecture
and
is
focused
on
management
of
connected
serial
consoles.
</p>


</div> <!-- ***************************** -->

<hr class='banner_sep' />

<div class='banner'>
<ul>
	<li><a href="../../plan_9_wiki/">Wiki</a></li>
	<li><a href="../../Documentation/">Docs</a></li>
	<li><a href="../../news/">News</a></li>
	<li><a href="../../development/">Devel</a></li>
	<li><a href="../../papers/">Papers</a></li>
	<li><a href="../../download/">Download</a></li>

</ul>
</div>

<hr class='footer_sep' />

<div class='footer'>
	<a href="../">Current Page</a> - 
	<span class='last_mod'>Last modified Sat Mar  2 23:10:45 EST 2013</span>
	
<br />
	<div class='about_wiki'>
<a href="http://plan9.bell-labs.com/plan9"><img align="middle" src="http://plan9.bell-labs.com/plan9/img/power36.gif" alt="Powered by Plan 9" /></a>
	<a href="../../about.html">About this Wiki</a>
	</div>


</div>

</body></html>
